{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c8ba7f",
   "metadata": {},
   "source": [
    "## Housing Price Predictor\n",
    "- Utilizes a dual model approach to predict the pricing of housing. \n",
    "- Created using data from H. Ahmed E. and Moustafa M. (2016). House Price Estimation from Visual and Textual Features.In Proceedings of the 8th International Joint Conference on Computational Intelligence (IJCCI 2016)ISBN 978-989-758-201-1, pages 62-68. DOI: 10.5220/0006040700620068\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877a362",
   "metadata": {},
   "source": [
    "## Helper Methods\n",
    "- Tools we may use later for data pre-processing and graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5908f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from helper_methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "# Plot an ROC. pred - the predictions, y - the expected output.\n",
    "def plot_roc(pred,y, file_path):\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50a2d5",
   "metadata": {},
   "source": [
    "## Set up Environment\n",
    "Create test output folders and define paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706beebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably safe to ignore the following error: \n",
      "[WinError 183] Cannot create a file when that file already exists: 'c:\\\\Users\\\\timef\\\\Documents\\\\Workspaces\\\\Python\\\\csc180\\\\housing-price-predictor\\\\test-output/'\n"
     ]
    }
   ],
   "source": [
    "base_path=os.path.join(os.getcwd(), 'test-output/')\n",
    "\n",
    "iteration='iteration-3'\n",
    "full_path = os.path.join(base_path, iteration)\n",
    "try:\n",
    "        os.mkdir(base_path)\n",
    "except Exception as e:\n",
    "     print(f\"Probably safe to ignore the following error: \\n{e}\")\n",
    "try:\n",
    "    os.mkdir(full_path)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\\nExiting to protect previous work.\")\n",
    "    sys.exit(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712d9a2",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "- code involved with the extraction of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f0b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "#reads text file data into dataframe (essentially a table)\n",
    "TXT_DATASET_LOCATION = \"./Houses Dataset/HousesInfo.txt\"\n",
    "\n",
    "cols=[\"Bedrooms\",\"Bathrooms\",\"area\",\"zipcode\",\"price\"]\n",
    "\n",
    "df = pd.read_csv(TXT_DATASET_LOCATION, sep= \" \", header=None, names=cols)\n",
    "# z-score encode and one-hot encode the features. 'price' is our output feature. \n",
    "encode_text_dummy(df, 'zipcode')\n",
    "encode_numeric_zscore(df, 'Bedrooms')\n",
    "encode_numeric_zscore(df, 'Bathrooms')\n",
    "encode_numeric_zscore(df, 'area')\n",
    "\n",
    "\n",
    "# retrieve \n",
    "\n",
    "#df.head()\n",
    "\n",
    "#\n",
    "\n",
    "# How do I read from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59f6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieval of images into arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ca22d",
   "metadata": {},
   "source": [
    "## Image Handling\n",
    "- Next we extract the images from the dataset and concatenate the 4 images in the dataset together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ddc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bathroom.jpg\n",
    "new_images=[]\n",
    "HOUSE_DATASET_LOCATION = \"./Houses dataset/\" \n",
    "for number in range(1, 536):\n",
    "    for path in glob.glob(\"./Houses dataset/\" + str(number) + \"_bathroom.jpg\"):\n",
    "        if os.path.isfile(path):\n",
    "            new_images.append(path) \n",
    "\n",
    "new_images\n",
    "\n",
    "# creates variable to store all the images into appropriate columns\n",
    "img= pd.DataFrame(new_images,columns = ['bathroom_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea9a5af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathroom_img</th>\n",
       "      <th>bedroom_img</th>\n",
       "      <th>frontal_img</th>\n",
       "      <th>kitchen_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./Houses dataset/1_bathroom.jpg</td>\n",
       "      <td>./Houses dataset/1_bedroom.jpg</td>\n",
       "      <td>./Houses dataset/1_frontal.jpg</td>\n",
       "      <td>./Houses dataset/1_kitchen.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./Houses dataset/2_bathroom.jpg</td>\n",
       "      <td>./Houses dataset/2_bedroom.jpg</td>\n",
       "      <td>./Houses dataset/2_frontal.jpg</td>\n",
       "      <td>./Houses dataset/2_kitchen.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./Houses dataset/3_bathroom.jpg</td>\n",
       "      <td>./Houses dataset/3_bedroom.jpg</td>\n",
       "      <td>./Houses dataset/3_frontal.jpg</td>\n",
       "      <td>./Houses dataset/3_kitchen.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./Houses dataset/4_bathroom.jpg</td>\n",
       "      <td>./Houses dataset/4_bedroom.jpg</td>\n",
       "      <td>./Houses dataset/4_frontal.jpg</td>\n",
       "      <td>./Houses dataset/4_kitchen.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./Houses dataset/5_bathroom.jpg</td>\n",
       "      <td>./Houses dataset/5_bedroom.jpg</td>\n",
       "      <td>./Houses dataset/5_frontal.jpg</td>\n",
       "      <td>./Houses dataset/5_kitchen.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bathroom_img                     bedroom_img  \\\n",
       "0  ./Houses dataset/1_bathroom.jpg  ./Houses dataset/1_bedroom.jpg   \n",
       "1  ./Houses dataset/2_bathroom.jpg  ./Houses dataset/2_bedroom.jpg   \n",
       "2  ./Houses dataset/3_bathroom.jpg  ./Houses dataset/3_bedroom.jpg   \n",
       "3  ./Houses dataset/4_bathroom.jpg  ./Houses dataset/4_bedroom.jpg   \n",
       "4  ./Houses dataset/5_bathroom.jpg  ./Houses dataset/5_bedroom.jpg   \n",
       "\n",
       "                      frontal_img                     kitchen_img  \n",
       "0  ./Houses dataset/1_frontal.jpg  ./Houses dataset/1_kitchen.jpg  \n",
       "1  ./Houses dataset/2_frontal.jpg  ./Houses dataset/2_kitchen.jpg  \n",
       "2  ./Houses dataset/3_frontal.jpg  ./Houses dataset/3_kitchen.jpg  \n",
       "3  ./Houses dataset/4_frontal.jpg  ./Houses dataset/4_kitchen.jpg  \n",
       "4  ./Houses dataset/5_frontal.jpg  ./Houses dataset/5_kitchen.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## bedroom images\n",
    "bedroom_images = []\n",
    "for number in range(1, 536):\n",
    "    for path in glob.glob(HOUSE_DATASET_LOCATION + str(number) + \"_bedroom.jpg\"):\n",
    "        if os.path.isfile(path):\n",
    "            bedroom_images.append(path) \n",
    "\n",
    "frontal_images = []\n",
    "for number in range(1, 536):\n",
    "    for path in glob.glob(HOUSE_DATASET_LOCATION + str(number) + \"_frontal.jpg\"):\n",
    "        if os.path.isfile(path):\n",
    "            frontal_images.append(path) \n",
    "\n",
    "kitchen_images = []\n",
    "for number in range(1, 536):\n",
    "    for path in glob.glob(HOUSE_DATASET_LOCATION + str(number) + \"_kitchen.jpg\"):\n",
    "        if os.path.isfile(path):\n",
    "            kitchen_images.append(path) \n",
    "\n",
    "img['bedroom_img']=bedroom_images\n",
    "img['frontal_img']=frontal_images\n",
    "img['kitchen_img']=kitchen_images\n",
    "img.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bdcfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code which concatenates houses images into one image for each house\n",
    "images_output=[]\n",
    "for row_index,row in img.iterrows():\n",
    "            inputImages=[]\n",
    "            outputImage = np.zeros((128, 128, 3), dtype=\"uint8\")\n",
    "            image_temp1 = cv2.imread(row.bathroom_img)\n",
    "            image1 = cv2.resize(image_temp1, (64 , 64))\n",
    "            \n",
    "            image_temp2 = cv2.imread(row.bedroom_img)\n",
    "            image2 = cv2.resize(image_temp2, (64 , 64))\n",
    "            \n",
    "            image_temp3 = cv2.imread(row.frontal_img)\n",
    "            image3 = cv2.resize(image_temp3, (64 , 64))\n",
    "            \n",
    "            image_temp4 = cv2.imread(row.kitchen_img)\n",
    "            image4 = cv2.resize(image_temp4, (64 , 64))\n",
    "              \n",
    "            inputImages.append(image1)\n",
    "            inputImages.append(image2)\n",
    "            inputImages.append(image3)\n",
    "            inputImages.append(image4)\n",
    "            \n",
    "            outputImage[0:64, 0:64] = inputImages[0]\n",
    "            outputImage[0:64, 64:128] = inputImages[1]\n",
    "            outputImage[64:128, 64:128] = inputImages[2]\n",
    "            outputImage[64:128, 0:64] = inputImages[3]\n",
    "            \n",
    "            #uncomment this if you want to see a boatload of images that it is concatenating\n",
    "            images_output.append(outputImage)      \n",
    "img_array = np.asarray(images_output)\n",
    "img_array = img_array.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec6b08",
   "metadata": {},
   "source": [
    "#### Do split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df634bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = to_xy(df, 'price')\n",
    "x_train, x_test,img_train, img_test, y_train, y_test = train_test_split(x,img_array, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea6559",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "The model is comprised of two smaller models. One model is for processing images. THis one is a CNN. The other model is for processing the data about the house. This is a FCN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5295564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 0\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - loss: 3.1335 - val_loss: 0.2281\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - loss: 0.3549 - val_loss: 0.0820\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - loss: 0.0457 - val_loss: 0.0413\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0195 - val_loss: 0.0047\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 0.0046 - val_loss: 0.0033\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Training Model 1\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 5.9003 - val_loss: 0.3977\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.2889 - val_loss: 0.0238\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0310 - val_loss: 0.0174\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0082 - val_loss: 0.0048\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0050 - val_loss: 0.0034\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0038 - val_loss: 0.0033\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Training Model 2\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 1.1896 - val_loss: 0.0139\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0328 - val_loss: 0.0252\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - loss: 0.0096 - val_loss: 0.0028\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Training Model 3\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 4.1788 - val_loss: 0.0194\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0121 - val_loss: 0.0066\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Training Model 4\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 8.9835 - val_loss: 2.0659\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.6144 - val_loss: 0.0355\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0100 - val_loss: 0.0033\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=os.path.join(full_path, \"best_weights.keras\"), verbose=0, save_best_only=True) # save best model\n",
    "for i in range(5):\n",
    "    print(f\"Training Model {i}\")\n",
    "    # Housing Data FCNN\n",
    "\n",
    "    data_fcn_visible = Input(shape=(x_train.shape[1],))\n",
    "    hidden1=Dense(32, activation='relu')(data_fcn_visible)\n",
    "    hidden2 = Dense(16, activation='relu')(hidden1)\n",
    "\n",
    "    # Image CNN\n",
    "    image_cnn_visible = Input(shape=(128, 128, 3))\n",
    "    conv1 = Conv2D(32, kernel_size=(3,3), activation='relu')(image_cnn_visible)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "    flat = Flatten()(pool1)\n",
    "\n",
    "    merge = concatenate([hidden1, flat])\n",
    "    # interpretation model\n",
    "    dense1=(Dense(16, activation='relu'))(flat)\n",
    "    dense2 = Dense(16, activation='relu')(dense1)\n",
    "    outputs=(Dense(1))(dense2)\n",
    "    model = Model(inputs=[data_fcn_visible, image_cnn_visible], outputs=outputs)\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='min', restore_best_weights=True)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam') #not done here, change as see fit (is broken)\n",
    "    history = model.fit([x_train, img_train], y_train, validation_data=([x_test, img_test], y_test), epochs=100, callbacks=[checkpointer, monitor])\n",
    "    plot_losses(history=history, base_path=full_path, iteration=i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43063066",
   "metadata": {},
   "source": [
    "### Load, Predict, Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e1ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "(107, 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([x_test, img_test])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 4\u001b[0m pred_roc \u001b[38;5;241m=\u001b[39m prediction[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m plot_roc(pred_roc, y_true, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(full_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn-roc-curve.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "model.load_weights(os.path.join(full_path, 'best_weights.keras'))\n",
    "prediction = model.predict([x_test, img_test])\n",
    "print(prediction.shape)\n",
    "pred_roc = prediction[:,0]\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "plot_roc(pred_roc, y_true, os.path.join(full_path, 'cnn-roc-curve.png'))\n",
    "try:\n",
    "        pred = np.argmax(prediction, axis=1)\n",
    "        # confusion matrix\n",
    "        cm = confusion_matrix(y_true, pred)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm, ['normal', 'malicious'])\n",
    "        plt.savefig(os.path.join(full_path, 'cnn-confusion-matrix.png'))\n",
    "        plt.close()\n",
    "except Exception as e:\n",
    "        print(f\"error creating confusion matrix plot:\\n{e} \")\n",
    "try:\n",
    "        with open(os.path.join(full_path, 'cnn-metrics.txt'), 'x') as file:\n",
    "                prediction=np.argmax(prediction, axis=1)\n",
    "                y_true = np.argmax(y_test, axis=1)\n",
    "                accuracy_score = metrics.accuracy_score(y_true, prediction)\n",
    "                precision_score = metrics.precision_score(y_true, prediction, average='weighted')\n",
    "                recall_score = metrics.recall_score(y_true, prediction, average= \"weighted\")\n",
    "                fl_score = metrics.f1_score(y_true, prediction, average= \"weighted\")\n",
    "                file.write(f\"Accuracy Score: {accuracy_score}\\n\")\n",
    "                file.write(f\"Precision Score: {precision_score}\\n\")\n",
    "                file.write(\"Recall score: {}\\n\".format(recall_score))\n",
    "                file.write(\"F1 score: {}\\n\".format(fl_score))\n",
    "                log_loss = metrics.log_loss(y_test, prediction)\n",
    "                print(f\"Log Loss: {log_loss}\")                \n",
    "                file.write(str(metrics.classification_report(y_true, prediction)))\n",
    "                file.write(\"\\nNumpy array of predictions\\n\")\n",
    "                file.write(np.array_str(prediction[0:5]))\n",
    "                file.write(\"y_test:\\n\")\n",
    "                file.write(np.array2string(y_test[0:5]))\n",
    "except OSError as e:\n",
    "        print(f\"Error while writing model metrics: \\n{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
